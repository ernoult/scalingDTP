{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaner implementation of Difference Target Propagation (DTP)\n",
    "\n",
    "In this notebook, I propose a more elegant and generalizable implementation of DTP which essentially builds on customized backward. \n",
    "In the end, the gradients computed by DTP could be simply given by ```loss.backward()```.\n",
    "\n",
    "Note: although the code here is complete, I could not test it yet since the import of ```cudnn_convolution_transpose``` does not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.utils import _pair\n",
    "import numpy as np\n",
    "from torch.utils.cpp_extension import load\n",
    "\n",
    "# load the PyTorch extension\n",
    "cudnn_convolution = load(name=\"cudnn_convolution\", sources=[\"cudnn_convolution.cpp\"], verbose=True)\n",
    "cudnn_transpose_convolution = load(name=\"cudnn_convolution_transpose\", sources=[\"cudnn_convolution_transpose.cpp\"],verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging cudnn imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we just want to make sure the imported ```cudnn_convolution``` and ```cudnn_transpose_convolution``` work. Of course, this is just a sanity check: the final goal is to wrap them with a subclass of ```nn.Module``` to write customized layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy input, convolutional weights and bias\n",
    "weight = torch.zeros(64, 3, 5, 5).to('cuda')\n",
    "bias   = torch.zeros(64).to('cuda')\n",
    "\n",
    "stride   = (2, 2)\n",
    "padding  = (0, 0)\n",
    "output_padding  = (0, 0)\n",
    "dilation = (1, 1)\n",
    "groups   = 1\n",
    "\n",
    "\n",
    "input  = torch.zeros(128, 3, 32, 32).to('cuda')\n",
    "\n",
    "# compute the result of convolution\n",
    "output = cudnn_convolution.convolution(input, weight, bias, stride, padding, dilation, groups, False, False)\n",
    "\n",
    "# create dummy gradient w.r.t. the output\n",
    "grad_output = torch.zeros(128, 64, 14, 14).to('cuda')\n",
    "\n",
    "# compute the gradient w.r.t. the weights and input\n",
    "grad_weight = cudnn_convolution.convolution_backward_weight(input, weight.shape, grad_output, stride, padding, dilation, groups, False, False, False)\n",
    "grad_input  = cudnn_convolution.convolution_backward_input(input.shape, weight, grad_output, stride, padding, dilation, groups, False, False, False)\n",
    "#grad_input = cudnn_convolution.convolution_transpose(grad_output, weight, bias, stride, padding, output_padding, dilation, groups, False, False)\n",
    "\n",
    "'''\n",
    "transpose_conv_output = cudnn_transpose_convolution.convolution_transpose(\n",
    "    grad_output,\n",
    "    weight,\n",
    "    bias,\n",
    "    padding,\n",
    "    output_padding,\n",
    "    stride,\n",
    "    dilation,\n",
    "    groups,\n",
    "    False,\n",
    "    False,\n",
    ")\n",
    "'''\n",
    "print(output.shape)\n",
    "print(grad_weight.shape)\n",
    "print(grad_input.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing fully connected autoencoders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define the ```neuron_fc``` class which is where we customize the ```backward``` for DTP. Then, we wrap it with the ```layer_fc``` class. \n",
    "\n",
    "**Important**: note that depending on whether we want to train the feedback weights (```w_b_learning=True```) or the feedforward weights (```back=True```), the ```backward``` method of ```neuron_fc``` behaves differently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the layer_fc class\n",
    "\n",
    "class neuron_fc(torch.autograd.Function):\n",
    "    '''\n",
    "    Defines a fully connected autoencoder with customized backward where:\n",
    "    F: x -> Linear layer\n",
    "    G: y -> Linear layer \n",
    "    (not activation function specified since it is meant to compute the logits of the \n",
    "    final classification layer before being passed through the softmax function)\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input, w_f, bias_f, w_b, bias_b, beta, w_b_learning, back, extra_input):\n",
    "        '''\n",
    "        w_f, bias_f: feedforward parameters\n",
    "        w_b, bias_b: feedback parameters\n",
    "        w_b_learning: specifies whether we want to train feedback weights\n",
    "        back: specifies whether we want to train the feedforward weights\n",
    "        beta: nudging strength in the output layer\n",
    "        extra_input: extra input injected in the output of the autoencoder (needed for feedback weights training)\n",
    "        '''\n",
    "        if w_b_learning:\n",
    "            if extra_input is not None:\n",
    "                y = extra_input\n",
    "            else:\n",
    "                input_flat = input.view(input.size(0), -1)\n",
    "                y = F.linear(input_flat, w_f, bias_f)\n",
    "            \n",
    "            r = F.linear(y, w_b, bias_b)\n",
    "            r = r.view(input.size())\n",
    "            ctx.save_for_backward(y)\n",
    "            ctx.w_b_learning = w_b_learning\n",
    "\n",
    "            return y, r\n",
    "\n",
    "        elif back:\n",
    "            input_flat = input.view(input.size(0), - 1)\n",
    "            y = F.linear(input_flat, w_f, bias_f)      \n",
    "            r = F.linear(y, w_b, bias_b)\n",
    "            r = r.view(input.size())\n",
    "            ctx.save_for_backward(input, y, r, w_b, bias_b)\n",
    "            ctx.beta = beta\n",
    "            ctx.w_b_learning = w_b_learning\n",
    "\n",
    "            return y\n",
    "\n",
    "        else:\n",
    "            input_flat = input.view(input.size(0), - 1)\n",
    "            y = F.linear(input_flat, w_f, bias_f)\n",
    "            return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_y, *args):        \n",
    "        '''\n",
    "        Note: the backward method changes according to whether \n",
    "        we train the feedback weights (w_b_learning=True) or\n",
    "        the feedforward weights otherwise (in which case we compute the targets)\n",
    "        '''\n",
    "        \n",
    "        w_b_learning = ctx.w_b_learning\n",
    "        \n",
    "        grad_beta = None\n",
    "        grad_w_b_learning = None\n",
    "        grad_back = None\n",
    "        grad_extra_input = None\n",
    "\n",
    "        if w_b_learning:\n",
    "            grad_r = args[0]\n",
    "            y = ctx.saved_tensors[0]\n",
    "            grad_input = None\n",
    "            grad_w_f = None\n",
    "            grad_bias_f = None\n",
    "            grad_r_flat = grad_r.view(grad_r.size(0), -1)\n",
    "            grad_w_b = grad_r_flat.t().mm(y)\n",
    "            grad_bias_b = grad_r_flat.sum(0).squeeze(0)\n",
    "\n",
    "        else:\n",
    "            input, y, r, w_b, bias_b = ctx.saved_tensors\n",
    "            beta = ctx.beta\n",
    "            \n",
    "\n",
    "            '''Computation of the first target'''\n",
    "            t = y + beta*grad_y\n",
    "            \n",
    "            r_perturb = F.linear(t, w_b, bias_b)\n",
    "            r_perturb = r_perturb.view(input.size())\n",
    "            grad_input = r_perturb - r\n",
    "            input_flat = input.view(input.size(0), - 1)\n",
    "            grad_w_f = grad_y.t().mm(input_flat)\n",
    "            grad_bias_f = grad_y.sum(0).squeeze(0)\n",
    "            grad_w_b = None\n",
    "            grad_bias_b = None\n",
    "\n",
    "        return grad_input, grad_w_f, grad_bias_f, grad_w_b, grad_bias_b, grad_beta, grad_w_b_learning, grad_back, grad_extra_input\n",
    "                   \n",
    "    \n",
    "\n",
    "class layer_fc(nn.Module):\n",
    "    '''\n",
    "    Defines the final class for fully connected autoencoders,\n",
    "    including most importantly the forward method and the method\n",
    "    to train the feedback weights\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, in_size, out_size, beta, iter, noise):\n",
    "        super(layer_fc, self).__init__()\n",
    "\n",
    "        self.beta = beta\n",
    "        self.iter = iter\n",
    "        self.noise = noise\n",
    "\n",
    "        self.w_f = nn.Parameter(torch.Tensor(out_size, in_size))\n",
    "        self.bias_f = nn.Parameter(torch.Tensor(out_size))\n",
    "        nn.init.normal_(self.w_f, 0, 0.01)\n",
    "        nn.init.constant_(self.bias_f, 0)\n",
    "        \n",
    "        self.w_b = nn.Parameter(torch.Tensor(in_size, out_size))\n",
    "        self.bias_b = nn.Parameter(torch.Tensor(in_size))\n",
    "        \n",
    "        nn.init.normal_(self.w_b, 0, 0.01)\n",
    "        nn.init.constant_(self.bias_b, 0)\n",
    "        \n",
    "\n",
    "    def forward(self, input, w_b_learning = False, back = False, extra_input = None):\n",
    "        return neuron_fc.apply(input, \n",
    "                               self.w_f, self.bias_f, \n",
    "                               self.w_b, self.bias_b, \n",
    "                               self.beta, w_b_learning, back, extra_input\n",
    "                              )\n",
    "\n",
    "    \n",
    "    def weight_b_train(self, input, optimizer, arg_return = False):\n",
    "        for iter in range(1, self.iter + 1):\n",
    "            \n",
    "            #Uncomment for sanity check: see if the angle between feedforward and feedback weights is decreasing\n",
    "            '''\n",
    "            if iter % 50 == 0:\n",
    "                dist, angle = self.compute_dist_angle()\n",
    "                print('\\n Step {}: Distance = {}, angle = {} \\n'.format(iter, dist, angle))\n",
    "            '''\n",
    "\n",
    "            \n",
    "            \n",
    "            y_temp, r_temp = self(input, w_b_learning = True)\n",
    "\n",
    "            noise = self.noise*torch.randn_like(input)\n",
    "            \n",
    "            y_noise, r_noise = self(input + noise, w_b_learning = True)\n",
    "            dy = y_noise - y_temp\n",
    "            dr = r_noise - r_temp\n",
    "            \n",
    "            noise_y = self.noise*torch.randn_like(y_temp)\n",
    "            \n",
    "            _, r_noise_y = self(input, extra_input = y_temp + noise_y, w_b_learning = True)\n",
    "            dr_y = (r_noise_y - r_temp)\n",
    "\n",
    "            \n",
    "            '''Loss of interest for the feedback weights'''\n",
    "            loss_b = -2*(noise*dr).view(dr.size(0), -1).sum(1).mean() + (dr_y**2).view(dr_y.size(0), -1).sum(1).mean() \n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "            loss_b.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Uncomment for sanity check: make sure that gradients do not propagate \"further than locally\"\n",
    "            '''\n",
    "            for name, param in self.named_parameters():\n",
    "                if param.grad is not None: print(name + ' has mean gradient {}'.format(param.grad.mean()))\n",
    "            '''\n",
    "  \n",
    "        if arg_return:\n",
    "            return loss_b\n",
    "\n",
    "    def compute_dist_angle(self, *args):\n",
    "        '''\n",
    "        Computes distance and angle\n",
    "        between feedforward and feedback weights\n",
    "        '''\n",
    "        F = self.w_f\n",
    "        G = self.w_b.t()\n",
    "\n",
    "        dist = torch.sqrt(((F - G)**2).sum()/(F**2).sum())\n",
    "\n",
    "        F_flat = torch.reshape(F, (F.size(0), -1))\n",
    "        G_flat = torch.reshape(G, (G.size(0), -1))\n",
    "        cos_angle = ((F_flat*G_flat).sum(1))/torch.sqrt(((F_flat**2).sum(1))*((G_flat**2).sum(1)))     \n",
    "        angle = (180.0/np.pi)*(torch.acos(cos_angle).mean().item())\n",
    "\n",
    "        return dist, angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we test the ```layer_fc``` class, in particular the forward pass and make sure that when running the algorithm on the feedback weights (**keeping feedforward weights fixed**), feedback weights come into alignment with feedforward weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the layer_fc class\n",
    "device = torch.device('cuda')\n",
    "\n",
    "batch_size = 128\n",
    "input_dim = 512\n",
    "output_dim = 10\n",
    "beta = 0.5\n",
    "iter = 200*50\n",
    "noise = 0.03\n",
    "lr = 0.035\n",
    "\n",
    "#1 - Instantiate layer_fc class\n",
    "dummy_layer = layer_fc(input_dim, output_dim, beta, iter, noise)\n",
    "dummy_layer.to(device)\n",
    "\n",
    "#2 - Forward pass\n",
    "dummy_input = torch.rand(batch_size, 512, 1, 1, requires_grad=True, device=device)\n",
    "print(dummy_input.shape)\n",
    "out = dummy_layer(dummy_input)\n",
    "\n",
    "#3 - Build optimizer for feedback weights\n",
    "my_list = ['w_b', 'bias_b']\n",
    "named_params_b = list(filter(lambda kv: kv[0] in my_list, dummy_layer.named_parameters()))\n",
    "params_b = []\n",
    "for name, param in named_params_b:\n",
    "    params_b.append(param)\n",
    "    print(name + ' has mean {}'.format(param.mean()))\n",
    "\n",
    "optim_params_b = [{'params': params_b, 'lr': lr}]\n",
    "optimizer_b = torch.optim.SGD(optim_params_b, momentum = 0.9)\n",
    "\n",
    "#4 - Train feedback weights\n",
    "dummy_layer.weight_b_train(dummy_input, optimizer_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing convolutional autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed in the exact same way to customize the conv layer.\n",
    "\n",
    "**Important remarks**: \n",
    "+ Some hyperparameters have been fixed within the class for simplicity for now, but in the future we will pass an argument parser into the constructor of the class to make it more general.\n",
    "+ In particular, the ELU function here is being used because, after careful experimental checking, it makes the algorithm on the feedback weights work much better than with the ReLU function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the layer_convpool class\n",
    "\n",
    "class neuron_convpool(torch.autograd.Function):\n",
    "    '''\n",
    "    Defines a convolutional autoencoder where:\n",
    "    F: x -> conv -> activation function -> pooling\n",
    "    G: y -> unpooling -> activation function -> convtranspose\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input, w_f, bias_f, \n",
    "                w_b, bias_b, \n",
    "                stride, padding, w_b_learning, back, extra_input):\n",
    "        \n",
    "        '''\n",
    "        Again here, we distinguish the two cases where we learn the feedback weights\n",
    "        (w_b_learning=True) of the feedforward weights otherwise. Depending on the case,\n",
    "        we do not store the same variables for the backward pass\n",
    "        '''\n",
    "        \n",
    "        if w_b_learning:\n",
    "            if extra_input is not None:\n",
    "                y, ind = extra_input\n",
    "            else:\n",
    "                y = cudnn_convolution.convolution(\n",
    "                    input, \n",
    "                    w_f, \n",
    "                    bias_f, \n",
    "                    stride,\n",
    "                    padding, \n",
    "                    (1, 1), \n",
    "                    1, \n",
    "                    False, \n",
    "                    False\n",
    "                )\n",
    "                \n",
    "                y, ind = F.max_pool2d(F.elu(y), 2, stride = 2, return_indices=True)\n",
    "\n",
    "            r = F.max_unpool2d(y, ind, 2, stride = 2, output_size = input.size())\n",
    "            r_prev = F.elu(r)\n",
    "                    \n",
    "            r = cudnn_transpose_convolution.convolution_transpose(\n",
    "                r_prev,\n",
    "                w_b,\n",
    "                bias_b, \n",
    "                stride, \n",
    "                padding_backward, \n",
    "                (1, 1),\n",
    "                1, \n",
    "                False, \n",
    "                False\n",
    "            )    \n",
    "            \n",
    "            ctx.save_for_backward(r_prev, w_b)\n",
    "            ctx.w_b_learning = w_b_learning\n",
    "            ctx.stride = stride\n",
    "            ctx.padding = padding\n",
    "            \n",
    "            return y, r, ind\n",
    "\n",
    "        elif back:\n",
    "            y = cudnn_convolution.convolution(\n",
    "                input, \n",
    "                w_f, \n",
    "                bias_f, \n",
    "                stride, \n",
    "                padding, \n",
    "                (1, 1),\n",
    "                1, \n",
    "                False,\n",
    "                False\n",
    "            )\n",
    "            \n",
    "            y, ind = F.max_pool2d(F.elu(y), 2, stride = 2, return_indices=True)\n",
    "            ctx.w_b_learning = w_b_learning\n",
    "            ctx.stride = stride\n",
    "            ctx.padding = padding\n",
    "            r_1 = F.elu(F.max_unpool2d(y, ind, 2, stride = 2, output_size = input.size()))\n",
    "            \n",
    "            if w_b is not None:\n",
    "                r_2 = cudnn_transpose_convolution.convolution_transpose(\n",
    "                    r_1,\n",
    "                    w_b, \n",
    "                    bias_b, \n",
    "                    stride, \n",
    "                    padding_backward, \n",
    "                    (1, 1),\n",
    "                    1, \n",
    "                    False, \n",
    "                    False\n",
    "                )\n",
    "                \n",
    "                ctx.save_for_backward(input, ind, r_1, r_2, w_b, bias_b, y)\n",
    "                \n",
    "            else:\n",
    "                ctx.save_for_backward(input, ind, r_1, w_f, y)\n",
    "            \n",
    "            return y\n",
    "\n",
    "        else:\n",
    "            y = cudnn_convolution.convolution(\n",
    "                input, \n",
    "                w_f, \n",
    "                bias_f,\n",
    "                stride, \n",
    "                padding, \n",
    "                (1, 1),\n",
    "                1, \n",
    "                False, \n",
    "                False\n",
    "            )\n",
    "            \n",
    "            y, ind = F.max_pool2d(F.elu(y), 2, stride = 2, return_indices=True)\n",
    "            return y\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_y, *args):\n",
    "        w_b_learning = ctx.w_b_learning\n",
    "        stride = ctx.stride\n",
    "        padding = ctx.padding\n",
    "        \n",
    "        grad_stride = None\n",
    "        grad_padding = None\n",
    "        grad_w_b_learning = None\n",
    "        grad_back = None\n",
    "        grad_extra_input = None\n",
    "\n",
    "        #If we train the feedback weights\n",
    "        if w_b_learning:\n",
    "            r_prev, w_b = ctx.saved_tensors\n",
    "            grad_r = args[0]\n",
    "            grad_input = None\n",
    "            grad_w_f = None\n",
    "            grad_bias_f = None\n",
    "            w_f_shape = torch.transpose(w_b, 0, 1).shape\n",
    "            grad_w_b = cudnn_transpose_convolution.convolution_transpose_backward_weight(\n",
    "                r_prev, \n",
    "                w_b.shape, \n",
    "                grad_r, \n",
    "                stride, \n",
    "                padding, \n",
    "                (1, 1), \n",
    "                1, \n",
    "                False, \n",
    "                False, \n",
    "                False\n",
    "            )\n",
    "            \n",
    "            grad_bias_b = torch.sum(grad_r, dim=[0, 2, 3]).squeeze(0)           \n",
    "            \n",
    "        #If we train the feedforward weights (and therefore compute the targets)\n",
    "        else:\n",
    "            grad_w_b = None\n",
    "            grad_bias_b = None\n",
    "            \n",
    "            if len(ctx.saved_tensors) > 4:\n",
    "                input, ind, r_1, r_2, w_b, bias_b, y = ctx.saved_tensors\n",
    "                \n",
    "                #Compute target\n",
    "                t = y + grad_y\n",
    "                \n",
    "                #Compute parameter gradient\n",
    "                r_perturb_1 = F.elu(\n",
    "                    F.max_unpool2d(\n",
    "                    t, \n",
    "                    ind, \n",
    "                    2, \n",
    "                    stride = 2, \n",
    "                    output_size = input.size()\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                delta_post_w_f = r_perturb_1 - r_1\n",
    "                \n",
    "                grad_w_f = cudnn_convolution.convolution_backward_weight(\n",
    "                    input,\n",
    "                    w_b.shape,\n",
    "                    delta_post_w_f,\n",
    "                    stride, \n",
    "                    padding, \n",
    "                    (1, 1), \n",
    "                    1,\n",
    "                    False, \n",
    "                    False, \n",
    "                    False\n",
    "                )\n",
    "                \n",
    "                grad_bias_f = torch.sum(delta_post_w_f, dim=[0, 2, 3]).squeeze(0)\n",
    "                \n",
    "                #Compute input gradient\n",
    "                r_perturb_2 = cudnn_transpose_convolution.convolution_transpose(\n",
    "                    r_perturb_1, \n",
    "                    w_b,\n",
    "                    bias_b, \n",
    "                    stride, \n",
    "                    padding_backward, \n",
    "                    (1, 1), \n",
    "                    1, \n",
    "                    False,\n",
    "                    False\n",
    "                )\n",
    "\n",
    "                grad_input = r_perturb_2 - r_2\n",
    "        \n",
    "            else:\n",
    "                input, ind, r_1, w_f, y = ctx.saved_tensors\n",
    "                \n",
    "                #Compute target\n",
    "                t = y + grad_y\n",
    "                \n",
    "                #Compute parameter gradient\n",
    "                r_perturb_1 = F.elu(\n",
    "                    F.max_unpool2d(\n",
    "                    t, \n",
    "                    ind, \n",
    "                    2, \n",
    "                    stride = 2, \n",
    "                    output_size = input.size()\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                delta_post_w_f = r_perturb_1 - r_1\n",
    "                \n",
    "                grad_w_f = cudnn_convolution.convolution_backward_weight(\n",
    "                    input,\n",
    "                    w_f.shape,\n",
    "                    delta_post_w_f,\n",
    "                    stride, \n",
    "                    padding, \n",
    "                    (1, 1), \n",
    "                    1,\n",
    "                    False, \n",
    "                    False, \n",
    "                    False\n",
    "                )\n",
    "                \n",
    "                grad_bias_f = torch.sum(delta_post_w_f, dim=[0, 2, 3]).squeeze(0)\n",
    "                grad_input = None\n",
    "            \n",
    "\n",
    "        return grad_input, grad_w_f, grad_bias_f, grad_w_b, grad_bias_b, grad_stride, grad_padding, grad_w_b_learning, grad_back, grad_extra_input\n",
    "          \n",
    "\n",
    "class layer_convpool(nn.Module):\n",
    "    '''\n",
    "    Defines the final class for convolutional autoencoders,\n",
    "    including most importantly the forward method and the method\n",
    "    to train the feedback weights\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 in_channels,\n",
    "                 out_channels, \n",
    "                 kernel_size, \n",
    "                 stride, \n",
    "                 padding, \n",
    "                 activation,\n",
    "                 iter=None, \n",
    "                 noise=None\n",
    "                ):\n",
    "        \n",
    "        super(layer_convpool, self).__init__()\n",
    "        \n",
    "        kernel_size = _pair(kernel_size)\n",
    "        stride      = _pair(stride)\n",
    "        padding     = _pair(padding)\n",
    "        \n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.w_f = nn.Parameter(torch.Tensor(out_channels, in_channels, *kernel_size))\n",
    "        self.bias_f = nn.Parameter(torch.Tensor(out_channels))\n",
    "        nn.init.kaiming_normal_(self.w_f, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.constant_(self.bias_f, 0)\n",
    "\n",
    "        if iter is not None:\n",
    "            self.w_b = nn.Parameter(torch.Tensor(out_channels, in_channels, *kernel_size))\n",
    "            self.bias_b = nn.Parameter(torch.Tensor(in_channels))\n",
    "            nn.init.kaiming_normal_(self.w_b, mode='fan_in', nonlinearity='relu')\n",
    "            nn.init.constant_(self.bias_b, 0)\n",
    "            self.iter = iter\n",
    "            self.noise = noise\n",
    "        else:\n",
    "            self.w_b = None\n",
    "            self.bias_b = None\n",
    "            self.iter = None\n",
    "            self.noise = None\n",
    "               \n",
    "        if activation == 'elu':\n",
    "            self.rho = nn.ELU()\n",
    "        elif activation == 'relu':\n",
    "            self.rho = nn.ReLU()\n",
    "            \n",
    "        self.iter = iter\n",
    "        self.noise = noise\n",
    "        \n",
    "    def forward(self, input, w_b_learning = False, back = False, extra_input = None):\n",
    "        return neuron_convpool.apply(\n",
    "            input,\n",
    "            self.w_f, \n",
    "            self.bias_f, \n",
    "            self.w_b,\n",
    "            self.bias_b, \n",
    "            self.stride, \n",
    "            self.padding, \n",
    "            w_b_learning, \n",
    "            back, \n",
    "            extra_input\n",
    "        )\n",
    "                                     \n",
    "\n",
    "    def weight_b_train(self, input, optimizer, arg_return = False):\n",
    "        for iter in range(1, self.iter + 1):\n",
    "            \n",
    "            if iter % 50 == 0:\n",
    "                dist, angle = self.compute_dist_angle()\n",
    "                print('\\n Step {}: Distance = {}, angle = {} \\n'.format(iter, dist, angle))\n",
    "            \n",
    "            y_temp, r_temp, ind = self(input, w_b_learning = True)\n",
    "            noise = self.noise*torch.randn_like(input)\n",
    "            _, r_noise, _ = self(input + noise, w_b_learning = True)\n",
    "            dr = (r_noise - r_temp)\n",
    "       \n",
    "            noise_y = self.noise*torch.randn_like(y_temp)\n",
    "            _, r_noise_y, _ = self(input, \n",
    "                                   extra_input = (y_temp + noise_y, ind), \n",
    "                                   w_b_learning = True\n",
    "                                  )\n",
    "            \n",
    "            dr_y = (r_noise_y - r_temp)\n",
    "\n",
    "            loss_b = -2*(noise*dr).view(dr.size(0), -1).sum(1).mean() + (dr_y**2).view(dr_y.size(0), -1).sum(1).mean() \n",
    "            \n",
    "            optimizer.zero_grad() \n",
    "            loss_b.backward()            \n",
    "            optimizer.step()\n",
    "  \n",
    "        if arg_return:\n",
    "            return loss_b\n",
    "    \n",
    "    def compute_dist_angle(self):\n",
    "        \n",
    "        F = self.w_f\n",
    "        G = self.w_b\n",
    "        \n",
    "        dist = torch.sqrt(((F - G)**2).sum()/(F**2).sum())\n",
    "\n",
    "        F_flat = torch.reshape(F, (F.size(0), -1))\n",
    "        G_flat = torch.reshape(G, (G.size(0), -1))\n",
    "        #dist = torch.sqrt(((F_flat - G_flat)**2).sum()/(F_flat**2).sum())\n",
    "        cos_angle = ((F_flat*G_flat).sum(1))/torch.sqrt(((F_flat**2).sum(1))*((G_flat**2).sum(1)))     \n",
    "        angle = (180.0/np.pi)*(torch.acos(cos_angle).mean().item())\n",
    "        \n",
    "        return dist, angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise here, we do sanity checks, debugging the forward pass and making sure that the algorithm on the feedback weights works (keeping feedforward weights fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the layer_convpool class\n",
    "#torch.manual_seed(0)\n",
    "batch_size = 128\n",
    "padding = 1\n",
    "stride = 1\n",
    "kernel_size = 3\n",
    "in_channels = 128\n",
    "out_channels = 128*2\n",
    "in_size = 16\n",
    "activation = 'elu'\n",
    "iter = 200*30\n",
    "noise = 0.4\n",
    "lr = 1e-4\n",
    "\n",
    "#1 - Instantiate layer_fc class\n",
    "dummy_layer = layer_convpool(\n",
    "    in_channels,\n",
    "    out_channels, \n",
    "    kernel_size,\n",
    "    stride,\n",
    "    padding, \n",
    "    activation, \n",
    "    iter, \n",
    "    noise\n",
    "    )\n",
    "\n",
    "dummy_layer.to('cuda')\n",
    "\n",
    "#2 - Forward pass\n",
    "dummy_input = torch.randn(batch_size, in_channels, in_size, in_size).to('cuda')\n",
    "out = dummy_layer(dummy_input)\n",
    "\n",
    "#3 - Build optimizer for feedback weights\n",
    "my_list = ['w_b', 'bias_b']\n",
    "named_params_b = list(filter(lambda kv: kv[0] in my_list, dummy_layer.named_parameters()))\n",
    "params_b = []\n",
    "for name, param in named_params_b:\n",
    "    params_b.append(param)\n",
    "    #print(name + ' has mean {}'.format(param.mean()))\n",
    "\n",
    "optim_params_b = [{'params': params_b, 'lr': lr}]\n",
    "optimizer_b = torch.optim.SGD(optim_params_b, momentum = 0.9)\n",
    "\n",
    "#4 - Train feedback weights\n",
    "dummy_layer.weight_b_train(dummy_input, optimizer_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking several autoencoders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we stack several convolutional autoencoders and a final fully connected autoencoder to form a VGG-like architecture to be used on CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the small_VGG class\n",
    "\n",
    "class small_VGG(nn.Module):\n",
    "    def __init__(self, iter):\n",
    "        super(small_VGG, self).__init__()\n",
    "        \n",
    "        size = 32\n",
    "        \n",
    "        C = [3, 128, 128, 256, 256, 512]\n",
    "        #iter = [None, 20, 30, 35, 55, 20]\n",
    "        noise = [None, 0.4, 0.4, 0.2, 0.2, 0.08]\n",
    "        padding = 1\n",
    "        stride = 1\n",
    "        kernel_size = 3\n",
    "        activation = 'elu'\n",
    "        beta = 0.7\n",
    "        \n",
    "        layers = []\n",
    "\n",
    "        for i in range(len(C) - 1):\n",
    "            layers += [layer_convpool(C[i], \n",
    "                                      C[i + 1],\n",
    "                                      kernel_size, \n",
    "                                      stride, \n",
    "                                      padding, \n",
    "                                      activation, \n",
    "                                      iter[i], \n",
    "                                      noise[i])\n",
    "                      ]\n",
    "            \n",
    "            size = int(np.floor(size/2))\n",
    " \n",
    "        layers += [layer_fc((size**2)*C[-1], 10, beta, iter[-1], noise[-1])]\n",
    "\n",
    "        layers = nn.ModuleList(layers)\n",
    "        self.layers = layers\n",
    "        \n",
    "        \n",
    "    def weight_b_train(self, x, optimizer_b):\n",
    "        y = self.layers[0](x).detach()\n",
    "        for id_layer in range(len(self.layers) - 1):\n",
    "            print('Optimizing feedback weights of layer {}...'.format(id_layer + 1))\n",
    "            self.layers[id_layer + 1].weight_b_train(y, optimizer_b)\n",
    "            \n",
    "            #Uncomment to debug\n",
    "            '''\n",
    "            for name, param in self.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    print(name + ' has mean gradient: {}'.format(param.grad.mean()))\n",
    "                else:\n",
    "                    print(name + ' has None gradient')\n",
    "            '''\n",
    "            \n",
    "            y = self.layers[id_layer + 1](y).detach()\n",
    "            #self.zero_grad()\n",
    "            \n",
    "\n",
    "    def forward(self, x, back = False):\n",
    "        s = x\n",
    "        for i in range(len(self.layers)):\n",
    "            s  = self.layers[i](s, back = back)\n",
    "        return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we debug the resulting architecture (forward pass, backward pass, feedback weights training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the small_VGG class\n",
    "device = torch.device('cuda:0')\n",
    "net = small_VGG()\n",
    "net.to(device)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#Check model parameters\n",
    "for name, p in net.named_parameters():\n",
    "    print(name + ': {}'.format(p.size()))\n",
    "\n",
    "#Test forward pass\n",
    "batch_size = 128\n",
    "input_channels = 3\n",
    "input_size = 32\n",
    "dummy_input = torch.randn(128, input_channels, input_size, input_size).to(device)\n",
    "output = net(dummy_input, back=True)\n",
    "print(output.size())\n",
    "\n",
    "\n",
    "#Test backward pass\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "dummy_target = torch.randint(1, 10, (batch_size,)).to('cuda')\n",
    "loss = criterion(output.float(), dummy_target).squeeze()\n",
    "print(loss)\n",
    "loss.backward()\n",
    "\n",
    "for name, p in net.named_parameters():\n",
    "    if p.grad is not None:\n",
    "        print(name + ' has mean gradient: {}'.format(p.grad.mean()))\n",
    "\n",
    "#Test feedback weights training\n",
    "net.weight_b_train(dummy_input, optimizer_b)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training by DTP on CIFAR-10\n",
    "\n",
    "Lastly, we present the final piece to train the previous architecture with our algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load CIFAR-10\n",
    "batch_size = 128\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "transform_train = torchvision.transforms.Compose([torchvision.transforms.RandomHorizontalFlip(0.5),\n",
    "                                                          torchvision.transforms.RandomCrop(size=[32,32], padding=4, padding_mode='edge'),\n",
    "                                                          torchvision.transforms.ToTensor(), \n",
    "                                                          torchvision.transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), \n",
    "                                                                                           std=(3*0.2023, 3*0.1994, 3*0.2010)) ])    \n",
    "\n",
    "transform_test = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), \n",
    "                                                 torchvision.transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), \n",
    "                                                                                  std=(3*0.2023, 3*0.1994, 3*0.2010)) ]) \n",
    "\n",
    "cifar10_train_dset = torchvision.datasets.CIFAR10('./cifar10_pytorch', train=True, transform=transform_train, download=True)\n",
    "cifar10_test_dset = torchvision.datasets.CIFAR10('./cifar10_pytorch', train=False, transform=transform_test, download=True)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar10_train_dset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(cifar10_test_dset, batch_size=200, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test VGG training on CIFAR-10\n",
    "\n",
    "#1 - Build the net\n",
    "device = torch.device('cuda:0')\n",
    "iter = [None, 200, 300, 350, 550, 200]\n",
    "net = small_VGG(iter)\n",
    "net.to(device)\n",
    "\n",
    "\n",
    "#2 - Build optimizer for *feedback* weights\n",
    "lr_b = [1e-4, 3.5e-4, 8e-3, 8e-3, 0.18]\n",
    "\n",
    "optim_params_b = []\n",
    "for i in range(len(net.layers) - 1):\n",
    "    my_list = ['layers.' + str(i + 1) + '.w_b', 'layers.'+ str(i + 1) + '.bias_b']\n",
    "    named_params_b = list(filter(lambda kv: kv[0] in my_list, net.named_parameters()))\n",
    "    params_b = []\n",
    "    for name, param in named_params_b:\n",
    "        params_b.append(param)\n",
    "        #print(name)\n",
    "        \n",
    "    optim_params_b += [{'params': params_b, 'lr': lr_b[i]}] \n",
    "    \n",
    "optimizer_b = torch.optim.SGD(optim_params_b, momentum = 0.9)\n",
    "\n",
    "#3 - Build optimizer for *forward* weights\n",
    "lr_f = 0.08\n",
    "wdecay = 1e-4\n",
    "\n",
    "optim_params_f = []\n",
    "for i in range(len(net.layers)):\n",
    "    my_list = ['layers.' + str(i) + '.w_f', 'layers.'+ str(i) + '.bias_f']\n",
    "    named_params_f = list(filter(lambda kv: kv[0] in my_list, net.named_parameters()))\n",
    "    params_f = []\n",
    "    for name, param in named_params_f:\n",
    "        params_f.append(param)\n",
    "        #print(name)\n",
    "        \n",
    "    optim_params_f += [{'params': params_f, 'lr': lr_f}] \n",
    "    \n",
    "optimizer_f = torch.optim.SGD(optim_params_f, momentum = 0.9, weight_decay = wdecay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_f, 85, eta_min=1e-5)\n",
    "\n",
    "#4 - Define training criterion\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "#5 - Train the net on CIFAR-10 (should reach ~30% train accuracy at the end of the first epoch)\n",
    "net.train()\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        #Optimize feedback weights\n",
    "        net.weight_b_train(data, optimizer_b)\n",
    "\n",
    "        optimizer_b.zero_grad()\n",
    "\n",
    "        #Forward pass\n",
    "        output = net(data, back=True)\n",
    "\n",
    "        #Optimize forward weights\n",
    "        loss = criterion(output.float(), target).squeeze()\n",
    "        optimizer_f.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_f.step()\n",
    "\n",
    "        #Compute current accuracy\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = output.max(1)\n",
    "        targets = target\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        percent_trainset = (batch_idx + 1)/len(train_loader)*100\n",
    "        train_acc = (correct/total)*100\n",
    "        print('Train accuracy : {:.2f} % ({:.2f} % of training set)'.format(train_acc, percent_trainset))\n",
    "\n",
    "    #Update scheduler\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
